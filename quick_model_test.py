#!/usr/bin/env python3
"""
Qwen3-30B-A3B å¿«é€Ÿæ¨¡å‹æµ‹è¯•è„šæœ¬

ç”¨äºå¿«é€ŸéªŒè¯æ¨¡å‹æ˜¯å¦æ­£å¸¸åŠ è½½å’Œæ¨ç†
"""

import os
import time
import torch
from vllm import LLM, SamplingParams


def test_model_basic():
    """åŸºç¡€æ¨¡å‹æµ‹è¯•"""
    model_path = "./models/models--Qwen--Qwen3-30B-A3B/snapshots/ae659febe817e4b3ebd7355f47792725801204c9"
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not os.path.exists(model_path):
        print(f"é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return False
    
    # æ£€æŸ¥GPU
    gpu_count = torch.cuda.device_count()
    print(f"æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
    
    if gpu_count == 0:
        print("é”™è¯¯: æœªæ£€æµ‹åˆ°GPU")
        return False
    
    tensor_parallel_size = min(gpu_count, 8)
    print(f"ä½¿ç”¨ {tensor_parallel_size} ä¸ªGPUè¿›è¡Œæ¨ç†")
    
    try:
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        start_time = time.time()
        
        # åŠ è½½æ¨¡å‹
        llm = LLM(
            model=model_path,
            tensor_parallel_size=tensor_parallel_size,
            gpu_memory_utilization=0.5,  # è¿›ä¸€æ­¥é™ä½å†…å­˜ä½¿ç”¨
            max_model_len=2048,  # å‡å°‘å†…å­˜ä½¿ç”¨
            trust_remote_code=True,
            enforce_eager=True,  # ä½¿ç”¨eageræ¨¡å¼å‡å°‘å†…å­˜
            swap_space=2  # å‡å°‘swapç©ºé—´
        )
        
        load_time = time.time() - start_time
        print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}ç§’")
        
        # ç®€å•æµ‹è¯•
        test_prompts = [
            "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "è¯·å†™ä¸€ä¸ªPythonçš„Hello Worldç¨‹åºã€‚"
        ]
        
        sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.8,
            max_tokens=200
        )
        
        print("\nå¼€å§‹æ¨ç†æµ‹è¯•...")
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\næµ‹è¯• {i}: {prompt}")
            
            start_time = time.time()
            outputs = llm.generate([prompt], sampling_params)
            inference_time = time.time() - start_time
            
            response = outputs[0].outputs[0].text
            output_tokens = len(outputs[0].outputs[0].token_ids)
            throughput = output_tokens / inference_time
            
            print(f"å“åº”: {response[:100]}...")
            print(f"ç”Ÿæˆtokens: {output_tokens}, è€—æ—¶: {inference_time:.2f}s, ååé‡: {throughput:.2f} tokens/s")
        
        print("\nâœ… æ¨¡å‹æµ‹è¯•æˆåŠŸ!")
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("=" * 50)
    print("Qwen3-30B-A3B å¿«é€Ÿæ¨¡å‹æµ‹è¯•")
    print("=" * 50)
    
    success = test_model_basic()
    
    if success:
        print("\nğŸ‰ æ¨¡å‹å·¥ä½œæ­£å¸¸ï¼Œå¯ä»¥è¿è¡Œå®Œæ•´æµ‹è¯•è„šæœ¬!")
    else:
        print("\nâš ï¸  æ¨¡å‹æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç¯å¢ƒ")
